In this notebook, a GPT2 model will be finetuned on textual data collected from Law StackExchange.
The model which has 335 million parameters was finetuned on a dataset containing information from over 24,000 posts.
