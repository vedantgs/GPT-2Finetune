In this notebook, a GPT2 model will be finetuned on textual data collected from Law StackExchange.
The model which has 335 million parameters was finetuned on a dataset with 380 million parameters.
